# Humano no Loop: Design de Intera√ß√£o

## Vis√£o Geral

O sistema de QA Inteligente foi projetado para **assistir** o QA, n√£o substitu√≠-lo. O "humano no loop" √© essencial para:
- **Valida√ß√£o** de recomenda√ß√µes geradas
- **Contexto de neg√≥cio** que a m√°quina n√£o tem
- **Decis√µes estrat√©gicas** sobre prioriza√ß√£o
- **Ajustes finos** baseados em conhecimento t√°cito

**IMPORTANTE**: Este √© um **framework** usado por QAs que n√£o t√™m acesso a Cursor/VSCode. O fluxo completo considera:
- ‚úÖ Exporta√ß√£o manual do Splunk (humano no Splunk web)
- ‚úÖ Upload do arquivo via interface web (ou colocar em pasta e usar terminal)
- ‚úÖ Processamento autom√°tico (sistema processa)
- ‚úÖ Interpreta√ß√£o e decis√£o (humano l√™ resultados)

Veja [GUIA-USUARIO-FINAL.md](GUIA-USUARIO-FINAL.md) para o fluxo passo a passo do usu√°rio final.

## Pontos de Intera√ß√£o Humana

### 1. An√°lise Reativa - Valida√ß√£o de Prioridades

**Quando**: Ap√≥s processar m√©tricas do Splunk

**O que o sistema faz**:
- Processa m√©tricas automaticamente
- Gera lista de endpoints priorit√°rios
- Calcula scores de prioridade
- Identifica tend√™ncias e riscos

**O que o humano faz**:
- **Revisa** a lista de prioridades
- **Ajusta** scores baseado em contexto de neg√≥cio
- **Remove** falsos positivos (ex: endpoints legados que n√£o precisam de teste)
- **Adiciona** endpoints importantes que o sistema n√£o detectou

**Interface proposta**:
```bash
# Sistema gera relat√≥rio
python scripts/analyze_reactive_metrics.py data/splunk_exports/complete_6h.csv

# Humano revisa e ajusta
python scripts/interactive_priority_review.py --snapshot-id 2025-12-14_13-00-00
```

### 2. Gera√ß√£o de Testes - Aprova√ß√£o e Edi√ß√£o

**Quando**: Sistema sugere criar testes para endpoints cr√≠ticos

**O que o sistema faz**:
- Identifica endpoints que precisam de testes
- Gera sugest√µes de casos de teste
- Cria estrutura b√°sica de collection Postman

**O que o humano faz**:
- **Aprova** ou **rejeita** sugest√µes
- **Edita** casos de teste gerados
- **Adiciona** casos espec√≠ficos que o sistema n√£o pensou
- **Ajusta** dados de teste (ex: IDs espec√≠ficos, tokens)

**Interface proposta**:
```bash
# Sistema gera sugest√µes
python scripts/generate_test_suggestions.py --endpoint "/api/v3/quotes"

# Humano revisa interativamente
python scripts/interactive_test_generator.py --collection-id abc123
```

### 3. An√°lise de Tend√™ncias - Interpreta√ß√£o

**Quando**: Sistema detecta degrada√ß√£o ou melhoria

**O que o sistema faz**:
- Compara per√≠odos
- Identifica mudan√ßas significativas
- Gera alertas autom√°ticos

**O que o humano faz**:
- **Interpreta** o contexto (ex: "degrada√ß√£o esperada ap√≥s deploy")
- **Investiga** causas raiz
- **Decide** se a√ß√£o √© necess√°ria
- **Documenta** decis√µes e aprendizados

**Interface proposta**:
```bash
# Sistema mostra tend√™ncias
python scripts/analyze_reactive_metrics.py --compare

# Humano adiciona contexto
python scripts/add_trend_context.py --trend-id TREND-001 --note "Deploy realizado √†s 14h"
```

### 4. Integra√ß√£o com Postman - Revis√£o de Collections

**Quando**: Sistema cria/atualiza collections automaticamente

**O que o sistema faz**:
- Cria collection com testes sugeridos
- Organiza por prioridade
- Adiciona assertions b√°sicas

**O que o humano faz**:
- **Revisa** collection no Postman
- **Ajusta** assertions
- **Adiciona** vari√°veis de ambiente
- **Executa** testes manualmente primeiro
- **Aprova** para automa√ß√£o cont√≠nua

**Fluxo**:
1. Sistema cria collection `"Reactive Tests - 2025-12-14"`
2. Humano recebe notifica√ß√£o/relat√≥rio
3. Humano abre no Postman e revisa
4. Humano marca como "aprovado" ou "precisa ajuste"
5. Sistema pode re-gerar baseado em feedback

### 5. Decis√µes de Cobertura - Estrat√©gia

**Quando**: Sistema identifica gaps de cobertura

**O que o sistema faz**:
- Lista endpoints sem testes
- Prioriza por criticidade
- Sugere tipos de teste

**O que o humano faz**:
- **Decide** quais gaps s√£o aceit√°veis (ex: endpoints internos)
- **Prioriza** baseado em roadmap
- **Planeja** sprints de cobertura
- **Balanceia** esfor√ßo vs. valor

**Interface proposta**:
```bash
# Sistema mostra gaps
python scripts/analyze_coverage_gaps.py

# Humano marca decis√µes
python scripts/mark_coverage_decisions.py --endpoint "/api/internal/*" --decision "skip" --reason "Endpoints internos, baixa prioridade"
```

## Padr√µes de Intera√ß√£o

### Modo Interativo vs. Autom√°tico

**Modo Autom√°tico** (atual):
- Sistema processa e gera relat√≥rios
- Humano l√™ e decide a√ß√µes manualmente
- Sem feedback loop

**Modo Interativo** (proposto):
- Sistema gera sugest√µes
- Humano revisa e aprova/rejeita
- Sistema aprende com feedback
- Pr√≥ximas sugest√µes melhoram

### Feedback Loop

```
Sistema Gera ‚Üí Humano Revisa ‚Üí Humano Ajusta ‚Üí Sistema Aprende ‚Üí Pr√≥xima Gera√ß√£o Melhor
```

**Exemplos de feedback**:
- "Este endpoint n√£o √© cr√≠tico" ‚Üí Sistema reduz prioridade
- "Faltou testar este cen√°rio" ‚Üí Sistema adiciona ao template
- "Este alerta √© falso positivo" ‚Üí Sistema ajusta threshold

## Implementa√ß√£o Futura

### Fase 1: Revis√£o Interativa (Pr√≥xima)
- Scripts interativos para revisar prioridades
- Aprova√ß√£o/rejei√ß√£o de sugest√µes
- Salvar decis√µes humanas

### Fase 2: Feedback Loop
- Sistema aprende com decis√µes humanas
- Melhora sugest√µes ao longo do tempo
- Personaliza para contexto do projeto

### Fase 3: Interface Web (Futuro)
- Dashboard para revisar m√©tricas
- Interface drag-and-drop para prioriza√ß√£o
- Visualiza√ß√£o de tend√™ncias
- Aprova√ß√£o de testes em lote

## Exemplos de Uso

### Cen√°rio 1: Revis√£o Semanal de M√©tricas

```bash
# 1. Sistema processa m√©tricas
python scripts/analyze_reactive_metrics.py data/splunk_exports/weekly.csv

# 2. Humano revisa prioridades
python scripts/review_priorities.py --interactive

# 3. Sistema gera testes aprovados
python scripts/generate_approved_tests.py --priority-threshold 70
```

### Cen√°rio 2: Investiga√ß√£o de Degrada√ß√£o

```bash
# 1. Sistema detecta degrada√ß√£o
python scripts/analyze_reactive_metrics.py --compare

# 2. Humano investiga e documenta
python scripts/investigate_trend.py --trend-id TREND-001
# [Abre interface para adicionar notas, screenshots, links para tickets]

# 3. Sistema cria ticket no Jira (se aprovado)
python scripts/create_investigation_ticket.py --trend-id TREND-001 --approved
```

### Cen√°rio 3: Planejamento de Cobertura

```bash
# 1. Sistema mostra gaps
python scripts/analyze_coverage_gaps.py

# 2. Humano planeja sprint
python scripts/plan_coverage_sprint.py --interactive
# [Interface para selecionar endpoints, estimar esfor√ßo, criar tickets]

# 3. Sistema gera ACs e testes
python scripts/generate_sprint_tests.py --sprint-id SPRINT-123
```

## Princ√≠pios de Design

1. **Transpar√™ncia**: Sistema sempre mostra **por que** sugeriu algo
2. **Reversibilidade**: Humano pode **desfazer** qualquer a√ß√£o autom√°tica
3. **Controle**: Humano tem **controle total** sobre o que √© automatizado
4. **Aprendizado**: Sistema **melhora** com feedback humano
5. **Efici√™ncia**: Automa√ß√£o **acelera** trabalho humano, n√£o substitui

## Pr√≥ximos Passos

1. ‚úÖ Fluxo b√°sico funcionando (an√°lise reativa)
2. üîÑ **Implementar revis√£o interativa de prioridades**
3. üîÑ **Criar interface para aprova√ß√£o de testes**
4. üîÑ **Adicionar feedback loop b√°sico**
5. üîÑ **Integrar com Jira para criar tickets aprovados**

